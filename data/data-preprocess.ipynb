{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd #necessary to read .csv files\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#text-preprocessing\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Service</th>\n",
       "      <th>QouteText</th>\n",
       "      <th>Point</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1password</td>\n",
       "      <td>AgileBits, Inc.its subsidiaries, affiliates, a...</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1password</td>\n",
       "      <td>Your use of the Service is at your sole risk.T...</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500px</td>\n",
       "      <td>Your use of the Service is at your sole risk.T...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500px</td>\n",
       "      <td>ermination500px may terminate or suspend any a...</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500px</td>\n",
       "      <td>As a condition to using Services, you are requ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500px</td>\n",
       "      <td>500px reserves the right, at its sole discreti...</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>500px</td>\n",
       "      <td>YOU AND 500PX AGREE THAT ANY PROCEEDINGS TO RE...</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>500px</td>\n",
       "      <td>By posting Visual Content to the Site, you gra...</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>academia-edu</td>\n",
       "      <td>It also enables us to serve you advertising an...</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>airbnb</td>\n",
       "      <td>We may retain some of your personal informatio...</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Service                                          QouteText    Point\n",
       "0     1password  AgileBits, Inc.its subsidiaries, affiliates, a...      bad\n",
       "1     1password  Your use of the Service is at your sole risk.T...      bad\n",
       "2         500px  Your use of the Service is at your sole risk.T...  neutral\n",
       "3         500px  ermination500px may terminate or suspend any a...      bad\n",
       "4         500px  As a condition to using Services, you are requ...  neutral\n",
       "5         500px  500px reserves the right, at its sole discreti...      bad\n",
       "6         500px  YOU AND 500PX AGREE THAT ANY PROCEEDINGS TO RE...      bad\n",
       "7         500px  By posting Visual Content to the Site, you gra...      bad\n",
       "8  academia-edu  It also enables us to serve you advertising an...      bad\n",
       "9        airbnb  We may retain some of your personal informatio...      bad"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load all of our data\n",
    "#tldr_data and tosdr_data are dicts\n",
    "\n",
    "tldr_data_file = open('./tldrlegal.json',)\n",
    "tldr_data = json.load(tldr_data_file)\n",
    "\n",
    "tosdr_data_file = open('./tosdr.json',)\n",
    "tosdr_data = json.load(tosdr_data_file)\n",
    "\n",
    "privacy_data = pd.read_csv('./privacy_policies_raw.csv')\n",
    "privacy_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#punctuation_str = \"\\'!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\\'\"\n",
    "#print(\"corpora :\", lemmatizer.lemmatize(\"liked\"))\n",
    "def remove_punctuation(input_string):\n",
    "    res = re.sub(r'[^\\w\\s]', '', input_string)\n",
    "    return res\n",
    "\n",
    "#print(remove_punctuation(\"DAS: ABSCHELUCH!&*@# RITTER!!!!????,.\"))\n",
    "\n",
    "def make_lowercase(input_string):\n",
    "    return input_string.lower()\n",
    "\n",
    "def remove_stopwords(input_string):\n",
    "    tokens = word_tokenize(input_string)\n",
    "    output_string = [w for w in tokens if not w in stop_words]\n",
    "    return output_string\n",
    "\n",
    "def lemmatization(input_list):\n",
    "    output_string = \"\"\n",
    "    #temp = input_string.split()\n",
    "    for word in input_list:\n",
    "        #print(word)\n",
    "        lemmatized = lemmatizer.lemmatize(word)\n",
    "        output_string += lemmatized + \" \"\n",
    "    return output_string\n",
    "\n",
    "\n",
    "def preprocess(input_string):\n",
    "#     output = remove_punctuation(input_string)\n",
    "    output = input_string\n",
    "    output = make_lowercase(output)\n",
    "    output = remove_stopwords(output)\n",
    "    output = lemmatization(output)\n",
    "    return output\n",
    "\n",
    "def get_summary_from_json(json_file):\n",
    "    data = pd.DataFrame(columns=['original_text', 'reference_summary'])\n",
    "    #iterate through the text of the json files\n",
    "    for i, item in enumerate(json_file):\n",
    "        current_item = json_file[item]\n",
    "        original_text_preprocess = preprocess(current_item['original_text'])\n",
    "        reference_summary_preprocess = preprocess(current_item['reference_summary'])\n",
    "        data = data.append({'original_text':original_text_preprocess,\n",
    "                            'reference_summary':reference_summary_preprocess}, ignore_index=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'output' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-89117291d962>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_summary_from_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtldr_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./tldr/tldr.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mget_summary_from_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtosdr_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./tosdr/tosdr.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-97371ae4f83d>\u001b[0m in \u001b[0;36mget_summary_from_json\u001b[1;34m(json_file)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mcurrent_item\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0moriginal_text_preprocess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_item\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'original_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mreference_summary_preprocess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_item\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reference_summary'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         data = data.append({'original_text':original_text_preprocess,\n",
      "\u001b[1;32m<ipython-input-3-97371ae4f83d>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(input_string)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m#     output = remove_punctuation(input_string)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_lowercase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemmatization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'output' referenced before assignment"
     ]
    }
   ],
   "source": [
    "get_summary_from_json(tldr_data).to_csv('./tldr/tldr.csv', index=False)\n",
    "get_summary_from_json(tosdr_data).to_csv('./tosdr/tosdr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test train split\n",
    "tldr_df = pd.read_csv('./tldr/tldr.csv',)\n",
    "tldr_train, tldr_test = train_test_split(tldr_df, test_size=0.2)\n",
    "tldr_train, tldr_dev = train_test_split(tldr_train, test_size=0.2)\n",
    "\n",
    "tldr_train.to_csv('./tldr/tldr_train.csv', index=False)\n",
    "tldr_dev.to_csv('./tldr/tldr_dev.csv', index=False)\n",
    "tldr_test.to_csv('./tldr/tldr_test.csv', index=False)\n",
    "\n",
    "tosdr_df = pd.read_csv('./tosdr/tosdr.csv')\n",
    "tosdr_train, tosdr_test = train_test_split(tosdr_df, test_size = 0.2)\n",
    "tosdr_train, tosdr_dev = train_test_split(tosdr_train, test_size=0.2)\n",
    "\n",
    "tosdr_train.to_csv('./tosdr/tosdr_train.csv', index=False)\n",
    "tosdr_dev.to_csv('./tosdr/tosdr_dev.csv', index=False)\n",
    "tosdr_test.to_csv('./tosdr/tosdr_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['QouteText']\n",
      "['Service']\n",
      "(array(['QouteText'], dtype='<U1450'), array(['Service'], dtype='<U1450'))\n",
      "<__main__.privacyDataset object at 0x000001C8ECC804E0>\n"
     ]
    }
   ],
   "source": [
    "#Make a Dataset, then DataLoader out of the privacy dataset\n",
    "class privacyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        #data loading\n",
    "        xy = np.loadtxt('./privacy_policies_raw.csv', delimiter = \",\", dtype ='str', usecols=(0, 1), encoding =\"utf8\")\n",
    "        self.x = xy[:, 1:]#torch.from_numpy(xy[:, 1:])#convert to tensor\n",
    "        self.y = xy[:, [0]]#torch.from_numpy(xy[:, [0]])\n",
    "        self.n_samples = xy.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "        #dataset[0]\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "dataset = privacyDataset()\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features)\n",
    "print(labels)\n",
    "print(first_data)\n",
    "print(dataset)\n",
    "#why does this last 25 minutes\n",
    "# dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers = 2)\n",
    "# dataiterator = iter(dataloader)\n",
    "\n",
    "# data = dataiter.next()\n",
    "# features, labels = data\n",
    "# print(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ddc0aa8ee1b4c2f0cc1c246cd75f3aad5d81a26bba68b2c6f47a07a3b57c34f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
